{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE THE KID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import gc\n",
    "import os\n",
    "\n",
    "###Â MODELS\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier # requires additional library\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# TRIED BUT NOT USED\n",
    "#from lightgbm import LGBMClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "### sklearn utils\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (precision_score, recall_score, confusion_matrix,\n",
    "                             classification_report, accuracy_score, f1_score)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/your/path/to/this/project\"\n",
    "evaluation_dir = root_dir + \"/evaluations\"\n",
    "df = pd.read_excel(f\"{root_dir}/your/path/to/your/data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TUMOR_TYPE\"] = df[\"TUMOR_TYPE\"].apply(lambda x: 'MB' if x == \"MEDULLOBLASTOMA\" else x)\n",
    "df[\"TUMOR_TYPE\"] = df[\"TUMOR_TYPE\"].apply(lambda x: 'EP' if x == \"EPENDYMOMA\" else x)\n",
    "df[\"TUMOR_TYPE\"] = df[\"TUMOR_TYPE\"].apply(lambda x: 'PA' if x == \"PILOCYTIC ASTROCYTOMA\" else x)\n",
    "df[\"TUMOR_TYPE\"] = df[\"TUMOR_TYPE\"].apply(lambda x: 'BG' if x == \"GLIOMA\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Each Case\n",
    "pfdf_MB_EP = df[(df[\"TUMOR_TYPE\"] == 'MB') | (df[\"TUMOR_TYPE\"] == 'EP')]\n",
    "pfdf_MB_PA = df[(df[\"TUMOR_TYPE\"] == 'MB') | (df[\"TUMOR_TYPE\"] == 'PA')]\n",
    "pfdf_MB_BG = df[(df[\"TUMOR_TYPE\"] == 'MB') | (df[\"TUMOR_TYPE\"] == 'BG')]\n",
    "pfdf_EP_PA = df[(df[\"TUMOR_TYPE\"] == 'EP') | (df[\"TUMOR_TYPE\"] == 'PA')]\n",
    "pfdf_EP_BG = df[(df[\"TUMOR_TYPE\"] == 'EP') | (df[\"TUMOR_TYPE\"] == 'BG')]\n",
    "pfdf_PA_BG = df[(df[\"TUMOR_TYPE\"] == 'PA') | (df[\"TUMOR_TYPE\"] == 'BG')]\n",
    "pfdf_MB_EP_PA_BG = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## name dataframes for each case\n",
    "pfdf_MB_EP.name = 'MB_EP'\n",
    "pfdf_MB_PA.name = 'MB_PA'\n",
    "pfdf_MB_BG.name = 'MB_BG'\n",
    "pfdf_EP_PA.name = 'EP_PA'\n",
    "pfdf_EP_BG.name = 'EP_BG'\n",
    "pfdf_PA_BG.name = 'PA_BG'\n",
    "pfdf_MB_EP_PA_BG.name = 'MB_EP_PA_BG'\n",
    "\n",
    "# determine names for models\n",
    "svm = \"SVM\"\n",
    "lsvm = \"LSVM\"\n",
    "logreg = \"LR\"\n",
    "rf = \"RF\"\n",
    "dt = \"DT\"\n",
    "gbc = \"GBC\"\n",
    "cbc = \"CBC\"\n",
    "xgb = \"XGB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [svm, lsvm, logreg, rf, dt, gbc, cbc, xgb]\n",
    "\n",
    "dataset = [pfdf_MB_EP, pfdf_MB_PA, pfdf_MB_BG, pfdf_EP_PA,\n",
    "           pfdf_EP_BG, pfdf_PA_BG, pfdf_MB_EP_PA_BG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# NOTE: This is a helper function to print the feature coefficients\n",
    "#       of the logistic regression model.\n",
    "#\n",
    "# Inputs:\n",
    "#   model: the trained model\n",
    "#   df: the dataframe used to train the model\n",
    "#   path: the main path to save the plot to\n",
    "#\n",
    "#   This function can be used for any model that has a coef_ attribute, \n",
    "#   when feature_importances_ is not available.\n",
    "############################################################################\n",
    "\n",
    "def print_feature_coefficients(model, df, path):\n",
    "    \n",
    "    features = df.drop('TUMOR_TYPE', axis=1).columns\n",
    "    importances = pd.DataFrame(data={\n",
    "        'Attribute': features,\n",
    "        'Importance': model.coef_[0]\n",
    "    })\n",
    "\n",
    "    importances[\"Importance\"] = importances[\"Importance\"].to_numpy().astype(np.float)\n",
    "    importances = importances.sort_values(by='Importance', ascending=False)\n",
    "    indices = np.argsort(importances[\"Importance\"])\n",
    "\n",
    "    sns.set_style('dark', {'axes.grid' : False})\n",
    "    with sns.plotting_context(rc={\"axes.labelsize\":16, \"xtick.labelsize\":14, \"ytick.labelsize\":14}):\n",
    "        plt.barh(range(len(indices)), importances[\"Importance\"], color = sns.cubehelix_palette(18, start=.5, rot=-.5), align='center')\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.barh(range(len(indices)), importances[\"Importance\"], color = sns.cubehelix_palette(18, start=.5, rot=-.5), align='center')\n",
    "    plt.yticks(range(len(indices)), importances[\"Attribute\"], fontsize = 13)\n",
    "    plt.xlabel(\"Relative weight scores\", fontsize = 13, fontweight=\"bold\")\n",
    "\n",
    "    # SAVE BOTH PNG AND PDF\n",
    "    plt.savefig(os.path.join(path, df.name + '_' + model.name + '_impTable.png'), bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    plt.savefig(os.path.join(path, df.name + '_' + model.name + '_impTable.pdf'), bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "    ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# Note: This is a helper function to print the feature importances for traditional ML models.\n",
    "#\n",
    "# Inputs:\n",
    "#   model: the trained model\n",
    "#   df: the dataframe used to train the model\n",
    "#   path: the main path to save the plot to\n",
    "############################################################################################################\n",
    "\n",
    "def print_feature_importances(model, df, path):\n",
    "    feature_imp = pd.Series(model.feature_importances_, index = df.drop('TUMOR_TYPE', axis=1).columns).sort_values(ascending = False)\n",
    "    sns.set_style('dark', {'axes.grid' : False})\n",
    "    with sns.plotting_context(rc={\"axes.labelsize\":16, \"xtick.labelsize\":14, \"ytick.labelsize\":14}):\n",
    "        sns.barplot(x = feature_imp, y = feature_imp.index, palette = \"coolwarm\", alpha=1)\n",
    "    print(\"\\n\")\n",
    "    plt.xlabel(\"Relative scores\", fontsize = 13, fontweight=\"bold\")\n",
    "    plt.ylabel(\"Features\", fontsize = 13, fontweight=\"bold\")\n",
    "    plt.savefig(os.path.join(path, df.name + '_' + model.name + '_FeatureImportances.pdf'), bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    plt.savefig(os.path.join(path, df.name + '_' + model.name + '_FeatureImportances.png'), bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a helper function to print the results of the model\n",
    "# Inputs:\n",
    "#   y_test: the true labels\n",
    "#   y_pred: the predicted labels\n",
    "#   df: the dataframe used to train the model\n",
    "#   model: the trained model\n",
    "#   path: the main path to save the plot to\n",
    "\n",
    "\n",
    "def print_results(y_test, y_pred, df, model, path):\n",
    "    print('Precision: %.4f' % precision_score(y_test, y_pred, average='macro'))\n",
    "    print('Recall: %.4f' % recall_score(y_test, y_pred, average='macro'))\n",
    "    print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n",
    "    print('F1 Score: %.4f' % f1_score(y_test, y_pred, average='macro'))\n",
    "    print('\\n')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('\\n')\n",
    "    labels = np.unique(y_pred)\n",
    "    with sns.plotting_context(rc={\"axes.labelsize\":16, \"xtick.labelsize\":14, \"ytick.labelsize\":14}):\n",
    "        table = sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "                            xticklabels = labels,\n",
    "                            yticklabels = labels,\n",
    "                            annot=True, fmt='.0f', cmap='Purples',\n",
    "                            annot_kws={\n",
    "                            'fontsize': 16,\n",
    "                            'fontweight': 'bold'})\n",
    "\n",
    "    table.set_xlabel('\\nPredicted Values', fontdict=dict(weight='bold', size=14))\n",
    "    table.set_ylabel('Actual Values', fontdict=dict(weight='bold', size=14))\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    img_path = os.path.join(path, df.name + '_' + model.name + '_matrix.png') \n",
    "    pdf_path = os.path.join(path, df.name + '_' + model.name + '_matrix.pdf') \n",
    "    \n",
    "    plt.savefig(img_path, bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    plt.savefig(pdf_path, bbox_inches=\"tight\",\n",
    "                pad_inches=0.1, transparent=False, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is a helper function to obtain the results of a trained model.\n",
    "#      It splits the data into train and test sets, trains the model, and prints the results.\n",
    "\n",
    "def exec_model(model, df, path, state):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"TUMOR_TYPE\"], axis = 1), df[\"TUMOR_TYPE\"], \n",
    "                                                        test_size = 0.45, \n",
    "                                                        stratify = df[\"TUMOR_TYPE\"], \n",
    "                                                        random_state = state)\n",
    "\n",
    "    ##################################################################################\n",
    "    # Initilize & train models here because we'll loop over different random states\n",
    "    # NOTE: We're using the \"name\" attribute to store the name of the model,\n",
    "    #       so that we can use it later to save the results to a particular file belongs to that model.\n",
    "    ##################################################################################\n",
    "\n",
    "    if(model == \"SVM\"):\n",
    "        model = SVC(kernel='linear', random_state=state)\n",
    "        model.name = \"SVM\"\n",
    "\n",
    "    if(model == \"LSVM\"):\n",
    "        model = LinearSVC(random_state=state)\n",
    "        model.name = \"LSVM\"\n",
    "\n",
    "    if(model == \"LR\"):\n",
    "        model = LogisticRegression(random_state=state)\n",
    "        model.name = \"LR\"\n",
    "\n",
    "    if(model == \"RF\"):\n",
    "        model = RandomForestClassifier(random_state=state)\n",
    "        model.name = \"RF\"\n",
    "\n",
    "    if(model == \"DT\"):\n",
    "        model = DecisionTreeClassifier(random_state=state)\n",
    "        model.name = \"DT\"\n",
    "\n",
    "    if(model == \"GBC\"):\n",
    "        model = GradientBoostingClassifier(random_state=state)\n",
    "        model.name = \"GBC\"\n",
    "\n",
    "    if(model == \"CBC\"):\n",
    "        model = CatBoostClassifier(verbose = False, random_state=state)\n",
    "        model.name = \"CBC\"\n",
    "\n",
    "    if(model == 'XGB'):\n",
    "        model = XGBClassifier(random_state=state)\n",
    "        model.name = \"XGB\"\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "\n",
    "    pipe = make_pipeline(StandardScaler(), model)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    if model.name == 'XGB':\n",
    "        y_pred = le.inverse_transform(y_pred)\n",
    "\n",
    "    \n",
    "    scores = []\n",
    "    scores.append(precision_score(y_test, y_pred, average='macro'))\n",
    "    scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "    scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    \n",
    "    ################## CREATE FILE PATH IF NOT EXIST #######################\n",
    "    state_dir = os.path.join(path, \"random_state-\" + f\"{state}\")\n",
    "    data_name_dir = os.path.join(state_dir, df.name + f\"_rs{state}\") \n",
    "    figures_path = os.path.join(data_name_dir, \"figures\")\n",
    "    matrices_path = os.path.join(data_name_dir, \"confusion_matrices\")\n",
    "\n",
    "    try:\n",
    "        os.mkdir(path) \n",
    "    except OSError as error: \n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(state_dir) \n",
    "    except OSError as error: \n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        os.mkdir(data_name_dir) \n",
    "    except OSError as error: \n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(figures_path)\n",
    "    except OSError as error: \n",
    "        pass \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(matrices_path) \n",
    "    except OSError as error: \n",
    "        pass\n",
    "\n",
    "    ######################################################################\n",
    "    # PRINT RESULTS\n",
    "    print_results(y_test, y_pred, df, model, matrices_path)\n",
    "\n",
    "    importance = np.zeros(X_train.columns.size,)\n",
    "\n",
    "    if (model.name == 'RF' or model.name == 'GBC' or model.name == 'XGB' or model.name == 'DT' or model.name == 'CBC'):\n",
    "        print_feature_importances(model, df, figures_path)\n",
    "        importance = np.array(model.feature_importances_)\n",
    "        importance = np.reshape(importance, (1, X_train.columns.size))\n",
    "    elif model.name == 'LR':\n",
    "        print_feature_coefficients(model, df, figures_path)\n",
    "    elif model.name == 'SVM':\n",
    "        print_feature_coefficients(model, df, figures_path)\n",
    "    elif model.name == 'LSVM':\n",
    "        print_feature_coefficients(model, df, figures_path)\n",
    "\n",
    "    if((importance == 0).all()):\n",
    "        importance = np.reshape(importance, (1, X_train.columns.size))\n",
    "        importance_df = pd.DataFrame(data=importance, columns=X_train.columns)\n",
    "    else:\n",
    "        importance_df = pd.DataFrame(data=importance, columns=X_train.columns)\n",
    "\n",
    "    return importance_df, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Note: This is main function that loops over the different datasets and models.\n",
    "#       It calls the exec_model function to train and evaluate the models.\n",
    "#       It also saves the results to a file.\n",
    "#       The results are saved in a directory called \"evaluation\" in the current working directory.\n",
    "####################################################################################################\n",
    "\n",
    "random_state_list = [1,42,123,1234,12345]\n",
    "for state in random_state_list:\n",
    "    for data in dataset:\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        importances = pd.DataFrame(data=None, columns=df.drop([\"TUMOR_TYPE\"], axis = 1).columns)\n",
    "        score_table = pd.DataFrame({\"Models\": [\"svm\", \"lsvm\", \"logreg\", \"rf\", \"dt\", \"gbc\", \"cbc\", \"xgb\"],\n",
    "                                    \"Precision\": np.nan,\n",
    "                                    \"Recall\": np.nan,\n",
    "                                    \"Accuracy\": np.nan,\n",
    "                                    \"F1_Score\": np.nan})\n",
    "\n",
    "        \n",
    "        for model in models:\n",
    "            print(\"\\n*********************************************************************************\" +\n",
    "                    \"\\n-------------- \" + data.name + \"  -  \" + model + \" --------------\\n\")\n",
    "            \n",
    "            importance_df, scores = exec_model(model, data, evaluation_dir, state)\n",
    "            precisions.append(scores[0])\n",
    "            recalls.append(scores[1])\n",
    "            accuracies.append(scores[2])\n",
    "            f1_scores.append(scores[3])\n",
    "            importances = pd.concat([importances, importance_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "        score_table[\"Precision\"] = precisions\n",
    "        score_table[\"Recall\"] = recalls\n",
    "        score_table[\"Accuracy\"] = accuracies\n",
    "        score_table[\"F1_Score\"] = f1_scores\n",
    "    \n",
    "        score_table = pd.concat([score_table, importances], axis = 1)\n",
    "\n",
    "        print(score_table)\n",
    "        out_path = os.path.join(evaluation_dir, f'random_state-{state}', data.name + f'_RS{state}.xlsx')\n",
    "        score_table.to_excel(out_path, encoding='utf-8')\n",
    "        del score_table, importances\n",
    "\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "036778c85436187ccf0576769abf132d07ad2ae14ef86fe4818ff7b09f64e9de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
